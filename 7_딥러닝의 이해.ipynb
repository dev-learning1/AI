{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"7_딥러닝의 이해.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMf770vAtVnyGuy7kE9rG9s"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["##1. 딥러닝의 진화\n","2012\n","- CNN중 Alex net이라는 네트워크 구조\n","- Alex net이 1등을 차지 -> CNN이 유명해지게 됨\n","\n","2013\n","- 아타리 게임 -> AI에 적용 -> 학습을 많이 진행 -> 구석을 파서 공을 위로 올리는 방법을 깨달음\n","- 딥마인드 -> 구글이 인수 -> 알파고\n","\n","2014\n","- RNN이라는 네트워크를 사용하여 중국어 -> 영어로 번역\n","- 성능의 한계가 생김\n","- attention 모델의 출현으로 성능이 급격히 좋아짐\n","\n","2015\n","- GANs -> 오바마 합성사진\n","- Residual Networks(ResNet) : CNN의 종류 중 하나\n","- 사람과 AI가 이미지를 찾는 테스트\n","- 사람은 5%오차률, ResNet 3%의 오차률\n","\n","2016\n","- 알파고 : 이세돌 9단이 컴퓨터를 이긴 마지막 인류\n","\n","2017\n","- RNN의 단점을 극복\n","- attention 만으로 만든 모델 -> Transformer : 번역 모델\n","- 자연어를 정복\n","- 딥러닝에서 가장 중요한 모델 중 하나\n","\n","2018\n","- Transformer에서 인코더만 따온 모델 -> BERT\n","- 자연어를 정답없이 사용(인터넷 문장들의 데이터를 삽입)\n","- 일부 글자를 가리고 가린 부분을 맞울 수 있도록 학습\n","- 모델을 크게 만들고 엄청난 데이터를 사용\n","\n","2019/2020\n","- GPU 수천대를 사용하여 큰 모델을 만듬\n","- 정답이 필요없이 실제 데이터만으로 셀프 수퍼바이즈러닝(강화학습)"],"metadata":{"id":"tlB5IrZ85qh9"}},{"cell_type":"markdown","source":["##2. 퍼셉트론(Perceptron)\n","- Perception : 인지하는 능력\n","- Neuron : 감각 입력 정보를 의미있는 정보로 바꿔주는 외에 있는 신경 세포\n","- Perceptron = Perception + Neuron. 생물학적 뉴런이 감각 정보를 받아서 문제를 해결하는 원리를 따라한 인공 뉴런"],"metadata":{"id":"lYHjhPe0E3IH"}},{"cell_type":"markdown","source":["##3. 딥러닝의 키포인트\n","- 데이터\n","- 모델(CNN, RNN, 트렌스포머, 멀티레이어 퍼셉트론...)\n","- 알고리즘(Gradient Descent를 기초로 많은 알고리즘이 만들어짐)\n","- loss function"],"metadata":{"id":"6wz9bqnvFyGQ"}},{"cell_type":"markdown","source":["##4. Neural Network에서의 핵심\n","- Deep Neural Network : layer 수가 최소 2개 이상인 network\n","- Loss Function / Cost Function : Neural Network가 얼마나 성능이 좋은지 또는 나쁜지에 대한 척도\n","- loss 값을 줄이는 방법 : 미분(접선의 기울기)이용\n",">적절한 weight(가중치)를 찾는 것이 핵심"],"metadata":{"id":"FkzyEbgiHLkd"}}]}